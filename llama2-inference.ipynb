{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":95581,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":80164,"modelId":104619}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate==0.33.0 transformers==4.31.0 tokenizers==0.13.3\n!pip install bitsandbytes==0.40.0 einops==0.6.1\n!pip install xformers==0.0.22.post7\n!pip install langchain==0.1.4\n!pip install faiss-gpu==1.7.1.post3\n!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:31:23.390578Z","iopub.execute_input":"2024-08-21T21:31:23.391336Z","iopub.status.idle":"2024-08-21T21:35:37.953155Z","shell.execute_reply.started":"2024-08-21T21:31:23.391306Z","shell.execute_reply":"2024-08-21T21:35:37.951990Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting accelerate==0.33.0\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nCollecting transformers==4.31.0\n  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers==0.13.3\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (2.1.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (0.23.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (3.13.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.33.0) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\nDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers, accelerate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.32.1\n    Uninstalling accelerate-0.32.1:\n      Successfully uninstalled accelerate-0.32.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.33.0 tokenizers-0.13.3 transformers-4.31.0\nCollecting bitsandbytes==0.40.0\n  Downloading bitsandbytes-0.40.0-py3-none-any.whl.metadata (9.8 kB)\nCollecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\nDownloading bitsandbytes-0.40.0-py3-none-any.whl (91.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, einops\nSuccessfully installed bitsandbytes-0.40.0 einops-0.6.1\nCollecting xformers==0.0.22.post7\n  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.22.post7) (1.26.4)\nCollecting torch==2.1.0 (from xformers==0.0.22.post7)\n  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->xformers==0.0.22.post7) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->xformers==0.0.22.post7) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->xformers==0.0.22.post7) (2024.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.1.0 (from torch==2.1.0->xformers==0.0.22.post7)\n  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->xformers==0.0.22.post7)\n  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0->xformers==0.0.22.post7) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0->xformers==0.0.22.post7) (1.3.0)\nDownloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 triton-2.1.0 xformers-0.0.22.post7\nCollecting langchain==0.1.4\n  Downloading langchain-0.1.4-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (0.6.7)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (1.33)\nCollecting langchain-community<0.1,>=0.0.14 (from langchain==0.1.4)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.4)\n  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\nCollecting langsmith<0.1,>=0.0.83 (from langchain==0.1.4)\n  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (2.32.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.4) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (3.21.3)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.4) (2.4)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-community<0.1,>=0.0.14 (from langchain==0.1.4)\n  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\nINFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.4)\n  Downloading langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\nINFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (4.2.0)\n  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\nCollecting langsmith<0.1,>=0.0.83 (from langchain==0.1.4)\n  Downloading langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.16->langchain==0.1.4)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.4) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.4) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.4) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.4) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.4) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.4) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.4) (2024.7.4)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.4) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.4) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.4) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (1.0.0)\nDownloading langchain-0.1.4-py3-none-any.whl (803 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langsmith, langchain-core, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.4 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 packaging-23.2\nCollecting faiss-gpu==1.7.1.post3\n  Downloading faiss_gpu-1.7.1.post3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.1.post3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (90.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.1.post3\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nCollecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.1.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.23.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.6.20)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.44.1-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers, sentence_transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.31.0\n    Uninstalling transformers-4.31.0:\n      Successfully uninstalled transformers-4.31.0\nSuccessfully installed sentence_transformers-3.0.1 tokenizers-0.19.1 transformers-4.44.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:35:37.955613Z","iopub.execute_input":"2024-08-21T21:35:37.956487Z","iopub.status.idle":"2024-08-21T21:35:40.624942Z","shell.execute_reply.started":"2024-08-21T21:35:37.956444Z","shell.execute_reply":"2024-08-21T21:35:40.624181Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:35:40.629600Z","iopub.execute_input":"2024-08-21T21:35:40.629867Z","iopub.status.idle":"2024-08-21T21:35:40.699148Z","shell.execute_reply.started":"2024-08-21T21:35:40.629843Z","shell.execute_reply":"2024-08-21T21:35:40.698261Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load the model with 4-bit Quantization","metadata":{}},{"cell_type":"code","source":"model_id = 'meta-llama/Llama-2-7b-chat-hf'","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:35:40.700192Z","iopub.execute_input":"2024-08-21T21:35:40.700469Z","iopub.status.idle":"2024-08-21T21:35:40.704614Z","shell.execute_reply.started":"2024-08-21T21:35:40.700444Z","shell.execute_reply":"2024-08-21T21:35:40.703767Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## quantization\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:35:40.705780Z","iopub.execute_input":"2024-08-21T21:35:40.706090Z","iopub.status.idle":"2024-08-21T21:35:40.718751Z","shell.execute_reply.started":"2024-08-21T21:35:40.706067Z","shell.execute_reply":"2024-08-21T21:35:40.717957Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"hf_auth=\"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\"\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:35:40.719682Z","iopub.execute_input":"2024-08-21T21:35:40.719946Z","iopub.status.idle":"2024-08-21T21:35:41.091231Z","shell.execute_reply.started":"2024-08-21T21:35:40.719914Z","shell.execute_reply":"2024-08-21T21:35:41.090324Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e162a3083f44498a1e3e6765bec06f"}},"metadata":{}}]},{"cell_type":"code","source":"model = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map={'': 'cuda:0'},\n    use_auth_token=hf_auth\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:35:41.092362Z","iopub.execute_input":"2024-08-21T21:35:41.092668Z","iopub.status.idle":"2024-08-21T21:36:43.646201Z","shell.execute_reply.started":"2024-08-21T21:35:41.092644Z","shell.execute_reply":"2024-08-21T21:36:43.644755Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1e88304666b4fb7bf1a66117346f5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6f511feb37431c96b4b77a91f36246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96391ab0a77645f6860af35f63e75d1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d4d09f7c91243bc864a06281c2f370c"}},"metadata":{}},{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 6.0\nCUDA SETUP: Detected CUDA version 121\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n  warn(msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60e60f060c7d4a93a4475aded3e62bf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c63f6698d7455eb481fb803868bed7"}},"metadata":{}},{"name":"stderr","text":"You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:36:43.647802Z","iopub.execute_input":"2024-08-21T21:36:43.648655Z","iopub.status.idle":"2024-08-21T21:36:43.666207Z","shell.execute_reply.started":"2024-08-21T21:36:43.648615Z","shell.execute_reply":"2024-08-21T21:36:43.665363Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:36:43.670477Z","iopub.execute_input":"2024-08-21T21:36:43.670748Z","iopub.status.idle":"2024-08-21T21:36:47.361759Z","shell.execute_reply.started":"2024-08-21T21:36:43.670725Z","shell.execute_reply":"2024-08-21T21:36:47.360715Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"169b02ce9aa14e39b33f508b9ae43b7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f8ee0f9fe8f4716bc1b2c5c184894c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"577aff599db1493faa4500450574b8e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8a255717da4b86ba4b116bfa1f4edc"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Define a Stopping Criteria","metadata":{}},{"cell_type":"code","source":"stop_list = ['\\nHuman:', '\\n```\\n']\n\nstop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\nstop_token_ids","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:36:47.363083Z","iopub.execute_input":"2024-08-21T21:36:47.363471Z","iopub.status.idle":"2024-08-21T21:36:47.374604Z","shell.execute_reply.started":"2024-08-21T21:36:47.363437Z","shell.execute_reply":"2024-08-21T21:36:47.373675Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nstop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\nstop_token_ids","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:36:47.377824Z","iopub.execute_input":"2024-08-21T21:36:47.378837Z","iopub.status.idle":"2024-08-21T21:36:50.398157Z","shell.execute_reply.started":"2024-08-21T21:36:47.378805Z","shell.execute_reply":"2024-08-21T21:36:50.397202Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import StoppingCriteria, StoppingCriteriaList\n\n# define custom stopping criteria object\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_ids in stop_token_ids:\n            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n                return True\n        return False\n\nstopping_criteria = StoppingCriteriaList([StopOnTokens()])","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:36:50.399509Z","iopub.execute_input":"2024-08-21T21:36:50.399939Z","iopub.status.idle":"2024-08-21T21:36:50.446319Z","shell.execute_reply.started":"2024-08-21T21:36:50.399907Z","shell.execute_reply":"2024-08-21T21:36:50.445331Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Hugging Face Pipeline","metadata":{}},{"cell_type":"code","source":"generate_text = transformers.pipeline(\n    model=model, \n    tokenizer=tokenizer,\n    return_full_text=True,  \n    task='text-generation',\n    stopping_criteria=stopping_criteria,  \n    temperature=0.1,  \n    max_new_tokens=512,  \n    repetition_penalty=1.1 \n)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:36:50.447583Z","iopub.execute_input":"2024-08-21T21:36:50.448435Z","iopub.status.idle":"2024-08-21T21:37:03.115845Z","shell.execute_reply.started":"2024-08-21T21:36:50.448388Z","shell.execute_reply":"2024-08-21T21:37:03.115048Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2024-08-21 21:36:52.864725: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-21 21:36:52.864825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-21 21:36:52.981226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"res = generate_text(\"What are different anomaly detection methods in finance\")\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:37:03.116908Z","iopub.execute_input":"2024-08-21T21:37:03.117509Z","iopub.status.idle":"2024-08-21T21:37:45.976597Z","shell.execute_reply.started":"2024-08-21T21:37:03.117480Z","shell.execute_reply":"2024-08-21T21:37:45.975660Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[{'generated_text': \"What are different anomaly detection methods in finance?\\n nobody likes to be surprised by unexpected losses or unexplained market movements. Anomaly detection is a crucial task for any financial institution, as it helps identify unusual patterns that may indicate potential risks or opportunities. Here are some common anomaly detection methods used in finance: 1. Statistical methods: These methods use statistical techniques such as mean reversion, standard deviation, and correlation analysis to detect anomalies. For example, if a stock's price deviates significantly from its historical mean, it may be considered an anomaly. 2. Machine learning methods: Machine learning algorithms can be trained on historical data to recognize patterns and detect anomalies. For instance, one could train a neural network to identify stock prices that are far away from their expected values. 3. Rule-based methods: These methods use predefined rules to identify anomalies based on specific criteria. For example, a rule might state that any trade that exceeds a certain percentage of the portfolio value is considered an anomaly. 4. Hybrid methods: These methods combine multiple approaches to detect anomalies. For instance, a hybrid method might use both machine learning and statistical techniques to identify anomalies. 5. Text analytics: This method uses natural language processing techniques to analyze text data such as financial news articles, social media posts, and company reports to identify sentiment changes or other anomalies. 6. Image recognition: This method uses image recognition techniques to analyze visual data such as charts, graphs, and financial reports to identify patterns and anomalies. 7. Time series analysis: This method uses time series analysis techniques to identify anomalies in financial data based on historical patterns and trends. 8. Clustering analysis: This method groups similar data points together and identifies those that do not fit into any cluster as anomalies. 9. Density-based methods: These methods use density functions to identify areas of high or low density in financial data, which can indicate anomalies. 10. Autoencoder-based methods: These methods use autoencoders to identify anomalies by comparing the reconstructed data to the original data. Each of these methods has its strengths and weaknesses, and the choice of method will depend on the nature of the financial data and the specific problem being addressed.\"}]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Langchain Pipeline","metadata":{}},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline(pipeline=generate_text)\n\nllm(prompt=\"What are different anomaly detection methods in finance\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:37:45.977634Z","iopub.execute_input":"2024-08-21T21:37:45.977895Z","iopub.status.idle":"2024-08-21T21:38:29.551826Z","shell.execute_reply.started":"2024-08-21T21:37:45.977873Z","shell.execute_reply":"2024-08-21T21:38:29.550813Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"\"?\\n nobody likes to be surprised by unexpected losses or unexplained market movements. Anomaly detection is a crucial task for any financial institution, as it helps identify unusual patterns that may indicate potential risks or opportunities. Here are some common anomaly detection methods used in finance: 1. Statistical methods: These methods use statistical techniques such as mean reversion, standard deviation, and correlation analysis to detect anomalies. For example, if a stock's price deviates significantly from its historical mean, it may be considered an anomaly. 2. Machine learning methods: Machine learning algorithms can be trained on historical data to recognize patterns and detect anomalies. For instance, one could train a neural network to identify stock prices that are far away from their predicted values. 3. Rule-based methods: These methods use predefined rules to identify anomalies based on specific criteria. For example, a rule might state that any trade that exceeds a certain percentage of the portfolio value is considered an anomaly. 4. Hybrid methods: These methods combine multiple approaches to detect anomalies. For instance, a hybrid approach might use both machine learning and statistical methods to identify anomalies. 5. Deep learning methods: Deep learning techniques such as Autoencoders, Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs) can be used to detect anomalies in financial time series data. 6. One-class SVM: One-class SVM is a type of machine learning algorithm that can be used to detect anomalies by training a model on normal data and then identifying instances that fall outside of the normal range. 7. Isolation Forest: Isolation Forest is an ensemble method that uses multiple decision trees to identify anomalies. It works by creating an isolation distance metric that measures how similar each data point is to its neighbors. 8. Local Outlier Factor (LOF): LOF is a density-based method that identifies anomalies by comparing the local density of each data point to the density of its neighbors. 9. Seasonal-Trend Decomposition (STL): STL is a time-series decomposition method that separates a signal into trend, seasonality, and residuals. Anomalies can be detected by analyzing the residuals. 10. Autoencoder-based\""},"metadata":{}}]},{"cell_type":"code","source":"from langchain.document_loaders import WebBaseLoader\n\nweb_links = [\"https://www.moneycontrol.com/news/business/tata-motors-mahindra-gain-certificates-for-production-linked-payouts-11281691.html\",\n             \"https://www.moneycontrol.com/news/business/tata-motors-launches-punch-icng-price-starts-at-rs-7-1-lakh-11098751.html\",\n             \"https://www.moneycontrol.com/news/business/stocks/buy-tata-motors-target-of-rs-743-kr-choksey-11080811.html\",\n             \"https://www.moneycontrol.com/news/business/markets/s-nasdaq-ekes-out-gain-even-as-alphabet-weighs-12797274.html\"] \n\nloader = WebBaseLoader(web_links)\ndocuments = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:38:29.553112Z","iopub.execute_input":"2024-08-21T21:38:29.553502Z","iopub.status.idle":"2024-08-21T21:38:32.913823Z","shell.execute_reply.started":"2024-08-21T21:38:29.553468Z","shell.execute_reply":"2024-08-21T21:38:32.912822Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:38:32.915249Z","iopub.execute_input":"2024-08-21T21:38:32.916274Z","iopub.status.idle":"2024-08-21T21:38:32.927673Z","shell.execute_reply.started":"2024-08-21T21:38:32.916234Z","shell.execute_reply":"2024-08-21T21:38:32.926773Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Faiss","metadata":{}},{"cell_type":"code","source":"from accelerate import Accelerator\naccelerator = Accelerator()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:38:32.928840Z","iopub.execute_input":"2024-08-21T21:38:32.929156Z","iopub.status.idle":"2024-08-21T21:38:32.971455Z","shell.execute_reply.started":"2024-08-21T21:38:32.929129Z","shell.execute_reply":"2024-08-21T21:38:32.970485Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\n\nmodel_name = \"ProsusAI/finbert\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# storing embeddings in the vector store\nvectorstore = FAISS.from_documents(all_splits, embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:40:00.038811Z","iopub.execute_input":"2024-08-21T21:40:00.039211Z","iopub.status.idle":"2024-08-21T21:40:04.274574Z","shell.execute_reply.started":"2024-08-21T21:40:00.039183Z","shell.execute_reply":"2024-08-21T21:40:04.273510Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b47fdecac76419fb523a2c0e2d1e3f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72760966dfe64b3ea822b16312dfc043"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14526edc300343bdba346467f000555d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d335708552e34235ae122b00f86e1aaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b0ca6ff797f409fa9796fff7f2d0792"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.chains import ConversationalRetrievalChain\n\nchain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:40:11.230541Z","iopub.execute_input":"2024-08-21T21:40:11.230999Z","iopub.status.idle":"2024-08-21T21:40:11.239527Z","shell.execute_reply.started":"2024-08-21T21:40:11.230964Z","shell.execute_reply":"2024-08-21T21:40:11.238498Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"chat_history = []\n\nquery = \"Explain KR Choksey's report on Tata Motors\"\nresult = chain({\"question\": query, \"chat_history\": chat_history})\n\nprint(result['answer'])","metadata":{"execution":{"iopub.status.busy":"2024-08-21T21:41:08.373742Z","iopub.execute_input":"2024-08-21T21:41:08.374130Z","iopub.status.idle":"2024-08-21T21:41:15.283839Z","shell.execute_reply.started":"2024-08-21T21:41:08.374101Z","shell.execute_reply":"2024-08-21T21:41:15.282904Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":" KR Choksey has given a buy recommendation on Tata Motors with a target price of Rs 743. The report suggests that the company's strong fundamentals and growth potential make it an attractive investment opportunity.\n","output_type":"stream"}]}]}